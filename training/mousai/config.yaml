# @package _global_

# python train.py exp=diff_lat_txt_0
module: main.module_diff_lat_txt_0 # TODO change to your module name
scratch_dir: /scratch # TODO change this to your scratch dir
#device: [0]
num_workers: 12
batch_size: 32

sampling_rate: 48000
length: 96000 # TODO, change it back to 2097152
channels: 2
log_every_n_steps: 1000
ckpt_every_n_steps: 5000

model:
  _target_: ${module}.Model
  lr: 1e-4
  lr_beta1: 0.95
  lr_beta2: 0.999
  lr_eps: 1e-6
  lr_weight_decay: 1e-3
  ema_beta: 0.995
  ema_power: 0.7
  embedding_mask_proba: 0.1 

  model:
    _target_: audio_diffusion_pytorch.models.DiffusionModel 
    in_channels: 32
    dim: 1 
    channels: [128, 256, 512, 512, 1024, 1024]
    factors: [1, 2, 2, 2, 2, 2]
    items: [2, 2, 2, 4, 8, 8]
    attentions: [0, 0, 1, 1, 1, 1]
    cross_attentions: [1, 1, 1, 1, 1, 1]
    attention_heads: 12
    attention_features: 64
    embedding_max_length: 64
    embedding_features: 768
    use_text_conditioning: True 
    use_embedding_cfg: True 

# TODO: change completely
datamodule:
  _target_: ${module}.Datamodule
  num_workers: ${num_workers}
  dataset_train:
    _target_: main.dataset.AudioWebDataset
    urls: pipe:aws s3 cp s3://s-harmonai/flavio/data/mix44s0/mix-{000000..000753}.tar -
    batch_size: ${batch_size}
  dataset_valid:
    _target_: main.dataset.AudioWebDataset
    urls: pipe:aws s3 cp s3://s-harmonai/flavio/data/mix44s0/mix-000755.tar -
    batch_size: ${batch_size}

callbacks:
  rich_progress_bar:
    _target_: pytorch_lightning.callbacks.RichProgressBar

  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    save_last: True 
    dirpath: /fsx/flavioschneider/ckpts/${now:%Y-%m-%d-%H-%M-%S}/ # TODO change this to your ckpt dir
    every_n_train_steps: ${ckpt_every_n_steps}

  model_summary:
    _target_: pytorch_lightning.callbacks.RichModelSummary
    max_depth: 2

  audio_samples_logger:
    _target_: ${module}.SampleLogger
    num_items: 3
    channels: ${channels}
    sampling_rate: ${sampling_rate}
    sampling_steps: [10, 50]
    decoder_sampling_steps: 50 
    embedding_scale: 5.0 
    use_ema_model: False

loggers:
  wandb:
    _target_: pytorch_lightning.loggers.wandb.WandbLogger
    project: ${oc.env:WANDB_PROJECT} # TODO add your wandb project name
    entity: ${oc.env:WANDB_ENTITY} # TODO add your wandb entity name
    # offline: False  # set True to store all logs only locally
    job_type: "train"
    group: ""
    save_dir: ${scratch_dir}

trainer:
  _target_: pytorch_lightning.Trainer
  precision: 32 # Precision used for tensors, default `32`
  accelerator: gpu 
  # devices: ${device}
  # strategy: ddp
  min_epochs: 0
  max_epochs: -1
  enable_model_summary: False
  log_every_n_steps: 1 # Logs metrics every N batches
  limit_val_batches: 10 
  check_val_every_n_epoch: null
  val_check_interval: ${log_every_n_steps}
  # default_root_dir: s3://s-harmonai/flavio/ckpts/${now:%Y-%m-%d-%H-%M-%S}/